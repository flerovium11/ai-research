{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train different Neural Networks on a Traffic Sign Detection Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import os\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataType(Enum):\n",
        "    TRAIN = 1\n",
        "    TEST = 2\n",
        "    VALID = 3\n",
        "\n",
        "class Label:\n",
        "    def __init__(self, raw_data: str) -> None:\n",
        "        split_data = raw_data.split(' ')\n",
        "        self.category = int(split_data[0])\n",
        "        self.center_x, self.center_y, self.width, self.height = map(float, split_data[1:])\n",
        "\n",
        "class Entry:\n",
        "    def __init__(self, image: np.array, labels: list[Label], image_name: str) -> None:\n",
        "        self.image = image\n",
        "        self.labels = labels\n",
        "        self.image_name = image_name\n",
        "\n",
        "\n",
        "dataset_path = 'data/traffic-signs-detection'\n",
        "info_file = os.path.join(dataset_path, 'car/data.yaml')\n",
        "categories = yaml.load(open(info_file), Loader=yaml.FullLoader)['names']\n",
        "\n",
        "img_size = 64\n",
        "min_bounding_box_size = 0.3\n",
        "only_single_label = True\n",
        "forbidden_file_prefixes = ['FisheyeCamera', 'road']\n",
        "grayscale = False\n",
        "\n",
        "def load_image_data(type: DataType):\n",
        "    data_path = os.path.join(dataset_path, 'car', type.name.lower())\n",
        "    images_path = os.path.join(data_path, 'images')\n",
        "    labels_path = os.path.join(data_path, 'labels')\n",
        "\n",
        "    entries = []\n",
        "    files_in_folder = os.listdir(images_path)\n",
        "    print(f'Scanning {len(files_in_folder)} entries from {images_path} and {labels_path}...')\n",
        "\n",
        "    for image_name in files_in_folder:\n",
        "        image = plt.imread(os.path.join(images_path, image_name))\n",
        "        image = np.array(Image.fromarray(image).resize((img_size, img_size)))\n",
        "\n",
        "        if grayscale:\n",
        "            image = np.mean(image, axis=2)\n",
        "        \n",
        "        image = image / 255.0\n",
        "        labels_raw = open(os.path.join(labels_path, image_name.replace('.jpg', '.txt'))).read().split('\\n')\n",
        "        labels = [Label(label) for label in labels_raw if label]\n",
        "\n",
        "        if (\n",
        "            only_single_label and len(labels) > 1\n",
        "            or len(labels) == 0\n",
        "            or any([image_name.startswith(prefix) for prefix in forbidden_file_prefixes])\n",
        "            or any([label.width < min_bounding_box_size or label.height < min_bounding_box_size for label in labels])\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        entries.append(Entry(image, labels, image_name))\n",
        "\n",
        "    return entries\n",
        "\n",
        "train_data = load_image_data(DataType.TRAIN)\n",
        "validate_data = load_image_data(DataType.VALID)\n",
        "test_data = load_image_data(DataType.TEST)\n",
        "\n",
        "print(f'Loaded {len(train_data)} training images, {len(validate_data)} validation images and {len(test_data)} test images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_images = 6\n",
        "categories_to_display = [0, 3, 13, 14]\n",
        "fig, axs = plt.subplots(len(categories_to_display), num_images, figsize=(num_images * 2, len(categories_to_display) * 2))\n",
        "print(f'Displaying {num_images} random images from each of the following categories: {\", \".join([categories[category] for category in categories_to_display])}')\n",
        "\n",
        "for idx, category in enumerate(categories_to_display):\n",
        "    category_entries = [entry for entry in validate_data if entry.labels[0].category == category]\n",
        "    \n",
        "    if len(category_entries) >= num_images:\n",
        "        selected_entries = random.sample(category_entries, num_images)\n",
        "    else:\n",
        "        selected_entries = category_entries\n",
        "    \n",
        "    for i, entry in enumerate(selected_entries):\n",
        "        axs[idx, i].imshow(entry.image, cmap='gray')\n",
        "        axs[idx, i].axis('off')\n",
        "        if i == 0:\n",
        "            axs[idx, i].set_title(categories[category])\n",
        "            \n",
        "        for label in entry.labels:\n",
        "            x = label.center_x * img_size\n",
        "            y = label.center_y * img_size\n",
        "            w = label.width * img_size\n",
        "            h = label.height * img_size\n",
        "            rect = plt.Rectangle((x - w / 2, y - h / 2), w, h, fill=False, color='r')\n",
        "            axs[idx, i].add_patch(rect)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, TypeVar\n",
        "\n",
        "Entry = TypeVar('Entry')\n",
        "\n",
        "def get_data(data: list[Entry]):\n",
        "    np.random.shuffle(data)\n",
        "    images = np.array([entry.image for entry in data])\n",
        "    labels = np.array([entry.labels[0].category for entry in data])\n",
        "    return images, labels\n",
        "\n",
        "# def print_category_distribution(labels):\n",
        "#     total = len(labels)\n",
        "#     distribution = {category: np.sum(labels == category) / total * 100 for category in np.unique(labels)}\n",
        "#     print(\", \".join([f'{categories[cat]}: {dist:.2f}%' for cat, dist in distribution.items()]))\n",
        "\n",
        "# def get_balanced_data(source_data: List[Entry], target_distribution: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "#     \"\"\"\n",
        "#     Resamples data to match a target distribution of categories.\n",
        "    \n",
        "#     Args:\n",
        "#         source_data: List of entries to be resampled\n",
        "#         target_distribution: Array of target percentages for each category\n",
        "        \n",
        "#     Returns:\n",
        "#         Tuple of (images, labels) with the desired distribution\n",
        "#     \"\"\"\n",
        "#     # Get initial data\n",
        "#     np.random.shuffle(source_data)\n",
        "#     images = np.array([entry.image for entry in source_data])\n",
        "#     labels = np.array([entry.labels[0].category for entry in source_data])\n",
        "    \n",
        "#     # Calculate current and target distributions\n",
        "#     n_categories = len(target_distribution)\n",
        "#     current_counts = np.array([np.sum(labels == i) for i in range(n_categories)])\n",
        "#     current_dist = current_counts / len(labels)\n",
        "    \n",
        "#     # Calculate number of samples needed for each category\n",
        "#     target_size = len(labels)\n",
        "#     target_counts = (target_distribution * target_size).astype(int)\n",
        "    \n",
        "#     # Initialize arrays for balanced dataset\n",
        "#     balanced_images = []\n",
        "#     balanced_labels = []\n",
        "    \n",
        "#     # Resample each category\n",
        "#     for category in range(n_categories):\n",
        "#         category_indices = np.where(labels == category)[0]\n",
        "        \n",
        "#         # If we need more samples than available, sample with replacement\n",
        "#         if target_counts[category] > len(category_indices):\n",
        "#             sampled_indices = np.random.choice(\n",
        "#                 category_indices,\n",
        "#                 size=target_counts[category],\n",
        "#                 replace=True\n",
        "#             )\n",
        "#         # If we need fewer samples, sample without replacement\n",
        "#         else:\n",
        "#             sampled_indices = np.random.choice(\n",
        "#                 category_indices,\n",
        "#                 size=target_counts[category],\n",
        "#                 replace=False\n",
        "#             )\n",
        "            \n",
        "#         balanced_images.extend(images[sampled_indices])\n",
        "#         balanced_labels.extend(labels[sampled_indices])\n",
        "    \n",
        "#     # Convert to numpy arrays and shuffle\n",
        "#     balanced_images = np.array(balanced_images)\n",
        "#     balanced_labels = np.array(balanced_labels)\n",
        "    \n",
        "#     # Shuffle the balanced dataset\n",
        "#     shuffle_idx = np.random.permutation(len(balanced_images))\n",
        "#     balanced_images = balanced_images[shuffle_idx]\n",
        "#     balanced_labels = balanced_labels[shuffle_idx]\n",
        "    \n",
        "#     return balanced_images, balanced_labels\n",
        "\n",
        "# # Calculate target distribution from training data\n",
        "# train_images, train_labels = get_data(train_data)\n",
        "# target_distribution = np.array([np.sum(train_labels == i) for i in range(len(categories))]) / len(train_labels)\n",
        "\n",
        "# # Resample validation and test data to match training distribution\n",
        "# validate_images, validate_labels = get_balanced_data(validate_data, target_distribution)\n",
        "# test_images, test_labels = get_balanced_data(test_data, target_distribution)\n",
        "\n",
        "# # Print distributions to verify\n",
        "# print(f'Category distribution in the training dataset ({len(train_images)} images):')\n",
        "# print_category_distribution(train_labels)\n",
        "# print(f'Category distribution in the validation dataset ({len(validate_images)} images):')\n",
        "# print_category_distribution(validate_labels)\n",
        "# print(f'Category distribution in the test dataset ({len(test_images)} images):')\n",
        "# print_category_distribution(test_labels)\n",
        "\n",
        "test_images, test_labels = get_data(test_data)\n",
        "validate_images, validate_labels = get_data(validate_data)\n",
        "train_images, train_labels = get_data(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "\n",
        "class CustomModel(kt.HyperModel):\n",
        "    def build(self, hp):\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = Sequential()\n",
        "\n",
        "        # Define input shape\n",
        "        model.add(Input(shape=(img_size, img_size, 3)))\n",
        "        model.add(Flatten())\n",
        "        \n",
        "        # Search for number of layers between 1 and 3\n",
        "        for i in range(hp.Int('num_layers', 1, 3)):\n",
        "            model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), activation='relu'))\n",
        "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0, max_value=0.5, step=0.05)))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(len(categories), activation='softmax'))\n",
        "\n",
        "        # Compile the model with an optimizer that has a tunable learning rate\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-6, max_value=1e-2, sampling='log')), \n",
        "                    loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "    \n",
        "    # thanks to https://github.com/keras-team/keras-tuner/issues/122#issuecomment-544648268\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        return model.fit(\n",
        "            *args,\n",
        "            batch_size=hp.Choice(\"batch_size\", [2 ** n for n in range(1, 11)]),\n",
        "            verbose=0,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "# Set up the tuner\n",
        "tuner = kt.Hyperband(\n",
        "    CustomModel(),\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=200,\n",
        "    hyperband_iterations=6,\n",
        "    directory='models',\n",
        "    project_name='ann_hyperparam_search'\n",
        ")\n",
        "\n",
        "class ClearGPUCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_end(self):\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "# Callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=12, min_lr=1e-6)\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
        "gpu_clear_callback = ClearGPUCallback()\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(train_images, train_labels,\n",
        "             validation_data=(validate_images, validate_labels),\n",
        "             callbacks=[lr_scheduler, early_stop])\n",
        "\n",
        "print('Search complete!')\n",
        "# Get the best model\n",
        "model = tuner.get_best_models()[0]\n",
        "print(model.summary())\n",
        "trials = tuner.oracle.get_best_trials()[:10]\n",
        "\n",
        "for trial in trials:\n",
        "    print(trial.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# validation accuracy would always stagnate at 75%, the model could go to 99% training accuracy with 4096 - 1024 - 256 but would overfit\n",
        "# that's why we need dropout but more neurons to compensate for the dropout, i don't wanna use l1/l2 or augmentation - just plain ann\n",
        "# training + validation accuracy would go to 70% with 6144 - 3072 - 1024 - 512 and 0.0001 lr, log curve and 150 epochs\n",
        "# now testing with 300 epochs and 0.0002 lr\n",
        "# lr 0.0002 didn't work, wonky and plateau at 50%, trying lr 0.0001 and 300 epochs -> still stopping at 70%\n",
        "# may try a different optimizer if my hyperparam search over 1e-2 - 1e-6 lr, 1-3 layers (0 - 0.5 dropout & 32-512 neurons each) doesnt work\n",
        "# split = int(len(train_data) * 0.8)\n",
        "# np.random.shuffle(train_data)\n",
        "# train_images, train_labels = get_data(train_data, None, split)\n",
        "# validate_images, validate_labels = get_data(train_data, split, None)\n",
        "\n",
        "model.add(Input(shape=(img_size, img_size, 3)))\n",
        "model.add(AveragePooling2D((3, 3), strides=3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Dense(len(categories), activation='softmax'))\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=12, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "            train_images,\n",
        "            train_labels,\n",
        "            batch_size=32,\n",
        "            epochs=300,\n",
        "            validation_data=(validate_images, validate_labels), \n",
        "            callbacks=[lr_scheduler])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout, AveragePooling2D, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(img_size, img_size, 3)))\n",
        "\n",
        "model.add(Conv2D(16, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(16, (5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(1e-4)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(len(categories), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, \n",
        "                    train_labels, \n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(validate_images, validate_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "plt.plot(history.history['accuracy'], color='red', label='Training')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label='Validierung')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochen')\n",
        "plt.ylabel('Genauigkeit')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def reshape_to_aspect(arr, aspect_ratio):\n",
        "    \"\"\"\n",
        "    Reshapes a 1D numpy array into a 2D grid that best matches the desired aspect ratio.\n",
        "    Adds padding (zeros) if necessary to fill the grid.\n",
        "    \n",
        "    Parameters:\n",
        "    arr (numpy.ndarray): 1D input array.\n",
        "    aspect_ratio (tuple): Desired aspect ratio as (width, height).\n",
        "    \n",
        "    Returns:\n",
        "    numpy.ndarray: 2D reshaped array with padding if necessary.\n",
        "    \"\"\"\n",
        "    if arr.size == 0:\n",
        "        return np.zeros((0, 0))\n",
        "    \n",
        "    w, h = aspect_ratio\n",
        "    target_ratio = w / h\n",
        "    N = arr.size\n",
        "    \n",
        "    # Calculate ideal number of rows based on target ratio\n",
        "    rows_ideal = np.sqrt(N / target_ratio)\n",
        "    \n",
        "    # Determine the range of rows to check around the ideal value\n",
        "    min_row = max(1, int(np.floor(rows_ideal)) - 3)\n",
        "    max_row = int(np.ceil(rows_ideal)) + 3\n",
        "    \n",
        "    best_diff = float('inf')\n",
        "    best_pad = float('inf')\n",
        "    best_shape = (1, N)  # Default shape, will be updated\n",
        "    \n",
        "    for row in range(min_row, max_row + 1):\n",
        "        if row == 0:\n",
        "            continue\n",
        "        cols = (N + row - 1) // row  # Ceiling division\n",
        "        actual_ratio = cols / row\n",
        "        ratio_diff = abs(actual_ratio - target_ratio)\n",
        "        padding = row * cols - N\n",
        "        \n",
        "        # Update best candidate if current is better\n",
        "        if (ratio_diff < best_diff) or (ratio_diff == best_diff and padding < best_pad):\n",
        "            best_diff = ratio_diff\n",
        "            best_pad = padding\n",
        "            best_shape = (row, cols)\n",
        "    \n",
        "    # Pad the array with zeros to fit the desired shape\n",
        "    padded = np.pad(arr, (0, best_shape[0] * best_shape[1] - N), mode='constant')\n",
        "    \n",
        "    return padded.reshape(best_shape)\n",
        "\n",
        "def visualize_model(test_image):\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Display input image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(test_image[0])\n",
        "    plt.title('Input Bild')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    for i, layer in enumerate(model.layers):    \n",
        "        if isinstance(layer, Conv2D):\n",
        "            output = Sequential(layers=model.layers[:i+1]).predict(test_image, verbose=0)\n",
        "            filters = layer.get_weights()[0]\n",
        "            n_filters = filters.shape[3]\n",
        "            \n",
        "            fig, axs = plt.subplots(2, n_filters, figsize=(n_filters * 2, 4))\n",
        "            \n",
        "            # Row 1: Filter weights\n",
        "            for i in range(n_filters):\n",
        "                filter_viz = np.mean(filters[:, :, :, i-1], axis=2)\n",
        "                axs[0, i].imshow(filter_viz, cmap='gray')\n",
        "                axs[0, i].set_title(f'Filter {i + 1} Gewichte')\n",
        "                axs[0, i].axis('off')\n",
        "            \n",
        "            # Row 2: Filter outputs\n",
        "            for i in range(n_filters):\n",
        "                axs[1, i].imshow(output[0, :, :, i], cmap='gray')\n",
        "                axs[1, i].set_title(f'Filter {i + 1} Output')\n",
        "                axs[1, i].axis('off')\n",
        "            \n",
        "            plt.suptitle(f'Layer: {layer.name} (Convolutional)', y=1.05)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        elif isinstance(layer, (MaxPooling2D, AveragePooling2D)):\n",
        "            output = Sequential(layers=model.layers[:i+1]).predict(test_image, verbose=0)\n",
        "            n_outputs = output.shape[-1]\n",
        "            \n",
        "            fig, axs = plt.subplots(1, n_outputs, figsize=(n_outputs * 2, 2))\n",
        "            \n",
        "            for j in range(n_outputs):\n",
        "                axs[j].imshow(output[0, :, :, j], cmap='gray' if output.shape[-1] != 3 else ['Reds', 'Greens', 'Blues'][j])\n",
        "                axs[j].set_title(f'Output {j + 1}')\n",
        "                axs[j].axis('off')\n",
        "            \n",
        "            pool_type = \"Max Pooling\" if isinstance(layer, MaxPooling2D) else \"Average Pooling\"\n",
        "            plt.suptitle(f'Schicht: {layer.name} ({pool_type})', y=1.05)\n",
        "            plt.tight_layout()\n",
        "            plt.show()   \n",
        "        \n",
        "        elif i == len(model.layers) - 1:\n",
        "            output = Sequential(layers=model.layers[:i+1]).predict(test_image, verbose=0)\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(5, 8))\n",
        "            ax.barh(np.arange(len(output[0])), output[0])\n",
        "            ax.set_yticks(np.arange(len(output[0])))\n",
        "            ax.set_yticklabels([categories[i] for i in range(len(output[0]))], rotation=45, ha='right')\n",
        "            ax.set_title('Output Klassenwahrscheinlichkeiten')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        \n",
        "                \n",
        "        elif isinstance(layer, Dense):\n",
        "            output = Sequential(layers=model.layers[:i+1]).predict(test_image, verbose=0)\n",
        "            image = reshape_to_aspect(output[0], (5, 1))\n",
        "            plt.title(f'Schicht: {layer.name} (Dense)')\n",
        "            plt.axis('off')\n",
        "            plt.imshow(image, cmap='gray')\n",
        "\n",
        "\n",
        "# find images from validation set that are misclassified\n",
        "misclassified = []\n",
        "for i, (image, label) in enumerate(zip(validate_images, validate_labels)):\n",
        "    prediction = model.predict(image[np.newaxis, ...])\n",
        "    predicted_label = np.argmax(prediction)\n",
        "    if predicted_label != label or True:\n",
        "        misclassified.append((image, label, predicted_label))\n",
        "        visualize_model(np.array([image]))\n",
        "        input('Weiter mit Enter')\n",
        "            \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "skipped = 0\n",
        "for entry in validate_data:\n",
        "    label = model.predict(np.array([entry.image]), verbose=0)[0].argmax()\n",
        "    if label == entry.labels[0].category:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    print(f'Skipped: {skipped} images')\n",
        "    skipped = 0\n",
        "    plt.imshow(entry.image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Predicted: {categories[label]}, Actual: {categories[entry.labels[0].category]}')\n",
        "    plt.show()\n",
        "    input('Press enter to continue...')\n",
        "    clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_filter(image: np.array, filter: np.array, stride: int = 1)->np.array:\n",
        "    image_copy = deepcopy(image)\n",
        "    filter_size = filter.shape[0]\n",
        "    image_size = image.shape[0]\n",
        "    for i in range(0, image_size - filter_size + 1, stride):\n",
        "        for j in range(0, image_size - filter_size + 1, stride):\n",
        "            image[i:i + filter_size, j:j + filter_size] = (image_copy[i:i + filter_size, j:j + filter_size] * filter).sum()\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "stop_signs = [entry for entry in validate_data if entry.labels[0].category == 14]\n",
        "grayscale_img = np.array(stop_signs[9].image).sum(axis=2) / 3\n",
        "vertical_edge_filter = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
        "horizontal_edge_filter = np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]])\n",
        "laplace_edge_filter = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]])\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(grayscale_img, cmap='gray')\n",
        "# plt.imsave('Stoppschild_laplace.png', grayscale_img, cmap='gray')\n",
        "\n",
        "# horizontal = apply_filter(deepcopy(grayscale_img),  horizontal_edge_filter, stride=1)\n",
        "# laplace = apply_filter(deepcopy(grayscale_img), laplace_edge_filter, stride=1)\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "# axs[0].imshow(horizontal, cmap='gray')\n",
        "# axs[0].axis('off')\n",
        "# axs[0].set_title('Horizontale Kanten')\n",
        "\n",
        "# axs[1].imshow(laplace, cmap='gray')\n",
        "# axs[1].axis('off')\n",
        "# axs[1].set_title('Laplace-Filter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the output value of the first layer of the custom model\n",
        "# print(model.layers)\n",
        "\n",
        "layer_models = []\n",
        "for i, layer in enumerate(model.layers):\n",
        "    layer_model = Sequential()\n",
        "    for j in range(i + 1):\n",
        "        layer_model.add(model.layers[j])\n",
        "    layer_models.append(layer_model)\n",
        "\n",
        "# layer_models[1].summary()\n",
        "\n",
        "# image = train_images[:1]\n",
        "# print(image.shape)\n",
        "# layer_models[1].predict(image)\n",
        "\n",
        "# raise\n",
        "\n",
        "# while True:\n",
        "#     # keras expects a batch of images, so we need to add an additional dimension to the image\n",
        "#     input_data = test_images[np.random.randint(0, len(test_images))][np.newaxis]\n",
        "#     layer_output = [layer_model.predict(input_data) for layer_model in layer_models]\n",
        "\n",
        "#     # the plot should look like this:\n",
        "#     #                        Layer 1 Filter 1 weights     Layer 1 Output 1       ...       Layer n Filter 1 weights     Layer n Output 1      \n",
        "#     # Original Image         ...                          ...                    ...       ...                          ...                     Predicted Category\n",
        "#     #                        Layer 1 Filter n weights     Layer 1 Output n       ...       Layer n Filter n weights     Layer n Output n\n",
        "#     #\n",
        "\n",
        "#     fig, ax, plt.subplots()\n",
        "\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "stop_signs = [entry for entry in validate_data if entry.labels[0].category == 14]\n",
        "\n",
        "# display the stop_signs[9].image in black and white and 28x28 pixels\n",
        "# right beside it display the same image but instead of pixels show values between 0 and 1 for the pixels\n",
        "size = 14\n",
        "grayscale_img = np.array(stop_signs[9].image).sum(axis=2) / 3\n",
        "scaled_img = np.array(Image.fromarray(grayscale_img).resize((size, size)))\n",
        "fig, axs = plt.subplots(1, 3, figsize=(16, 8))\n",
        "axs[0].imshow(grayscale_img, cmap='gray')\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Original (Schwarz-WeiÃŸ)')\n",
        "\n",
        "axs[1].imshow(scaled_img, cmap='gray')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Skaliert auf 14x14 Pixel')\n",
        "\n",
        "axs[2].imshow(scaled_img, cmap='gray')\n",
        "axs[2].axis('off')\n",
        "axs[2].set_title('Helligkeitswerte')\n",
        "\n",
        "for i in range(size):\n",
        "    for j in range(size):\n",
        "        axs[2].text(j, i, f'{scaled_img[i, j]:.2f}', ha='center', va='center', color='black' if scaled_img[i, j] > 0.5 else 'white', fontsize=7)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "input_data = test_images[np.random.randint(0, len(test_images))][np.newaxis]\n",
        "input_data = stop_signs[9].image[np.newaxis]\n",
        "layer_output = layer_models[0].predict(input_data)\n",
        "# plot the original image, the weights of the first kernel and the output of the first layer\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "axs[0].imshow(input_data[0])\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Original')\n",
        "\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "weights_min = weights.min()\n",
        "weights_max = weights.max()\n",
        "weights = (weights - weights_min) / (weights_max - weights_min)\n",
        "axs[1].imshow(weights[:, :, 0, 0], cmap='gray')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('First Kernel Weights')\n",
        "\n",
        "output = layer_output[0]\n",
        "axs[2].imshow(output[:, :, 0], cmap='gray')\n",
        "axs[2].axis('off')\n",
        "axs[2].set_title('Output of First Layer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What happened?\n",
        "1. I suffered from exploding gradients with HeInitialization() and MeanSquaredError(). I tried gradient clipping to 1.0 but turns out I don't have exploding gradients but vanishing gradients and exploding outputs. The outputs were very high with very low gradients, weird... Possible reasons are MeanSquaredError(), ReLU() and the batch size of 1 leading to instable gradients. Other potential reason: high learning rate.\n",
        "2. I changed to CrossEntropy() -> still vanishing gradients\n",
        "3. LeakyReLU() -> still vanishing\n",
        "4. Used RandomInitialization() and Sigmoid() activation function, works perfectly now\n",
        "\n",
        "I'm going to try adding batch processing so it will also work, maybe something with my Softmax output function was wrong, I want to try the original architecture (128 64 LeakyReLU, CrossEntropy, Xavier but with a different output activation function to see if the code for Softmax is broken)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
