{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train different Neural Networks on a Traffic Sign Detection Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import os\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataType(Enum):\n",
        "    TRAIN = 1\n",
        "    TEST = 2\n",
        "    VALID = 3\n",
        "\n",
        "class Label:\n",
        "    def __init__(self, raw_data: str) -> None:\n",
        "        split_data = raw_data.split(' ')\n",
        "        self.category = int(split_data[0])\n",
        "        self.center_x, self.center_y, self.width, self.height = map(float, split_data[1:])\n",
        "\n",
        "class Entry:\n",
        "    def __init__(self, image: np.array, labels: list[Label], image_name: str) -> None:\n",
        "        self.image = image\n",
        "        self.labels = labels\n",
        "        self.image_name = image_name\n",
        "\n",
        "\n",
        "dataset_path = 'data/traffic-signs-detection'\n",
        "info_file = os.path.join(dataset_path, 'car/data.yaml')\n",
        "categories = yaml.load(open(info_file), Loader=yaml.FullLoader)['names']\n",
        "\n",
        "img_size = 128\n",
        "min_bounding_box_size = 0.2\n",
        "only_single_label = True\n",
        "forbidden_file_prefixes = ['FisheyeCamera', 'road']\n",
        "grayscale = False\n",
        "\n",
        "def load_image_data(type: DataType):\n",
        "    data_path = os.path.join(dataset_path, 'car', type.name.lower())\n",
        "    images_path = os.path.join(data_path, 'images')\n",
        "    labels_path = os.path.join(data_path, 'labels')\n",
        "\n",
        "    entries = []\n",
        "    files_in_folder = os.listdir(images_path)\n",
        "    print(f'Scanning {len(files_in_folder)} entries from {images_path} and {labels_path}...')\n",
        "\n",
        "    for image_name in files_in_folder:\n",
        "        image = plt.imread(os.path.join(images_path, image_name))\n",
        "        image = np.array(Image.fromarray(image).resize((img_size, img_size)))\n",
        "\n",
        "        if grayscale:\n",
        "            image = np.mean(image, axis=2)\n",
        "        \n",
        "        image = image / 255.0\n",
        "        labels_raw = open(os.path.join(labels_path, image_name.replace('.jpg', '.txt'))).read().split('\\n')\n",
        "        labels = [Label(label) for label in labels_raw if label]\n",
        "\n",
        "        if (\n",
        "            only_single_label and len(labels) > 1\n",
        "            or len(labels) == 0\n",
        "            or any([image_name.startswith(prefix) for prefix in forbidden_file_prefixes])\n",
        "            or any([label.width < min_bounding_box_size or label.height < min_bounding_box_size for label in labels])\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        entries.append(Entry(image, labels, image_name))\n",
        "\n",
        "    return entries\n",
        "\n",
        "train_data = load_image_data(DataType.TRAIN)\n",
        "validate_data = load_image_data(DataType.VALID)\n",
        "test_data = load_image_data(DataType.TEST)\n",
        "\n",
        "print(f'Loaded {len(train_data)} training images, {len(validate_data)} validation images and {len(test_data)} test images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_images = 6\n",
        "categories_to_display = [0, 3, 13, 14]\n",
        "fig, axs = plt.subplots(len(categories_to_display), num_images, figsize=(num_images * 2, len(categories_to_display) * 2))\n",
        "print(f'Displaying {num_images} random images from each of the following categories: {\", \".join([categories[category] for category in categories_to_display])}')\n",
        "\n",
        "for idx, category in enumerate(categories_to_display):\n",
        "    category_entries = [entry for entry in validate_data if entry.labels[0].category == category]\n",
        "    \n",
        "    if len(category_entries) >= num_images:\n",
        "        selected_entries = random.sample(category_entries, num_images)\n",
        "    else:\n",
        "        selected_entries = category_entries\n",
        "    \n",
        "    for i, entry in enumerate(selected_entries):\n",
        "        axs[idx, i].imshow(entry.image, cmap='gray')\n",
        "        axs[idx, i].axis('off')\n",
        "        if i == 0:\n",
        "            axs[idx, i].set_title(categories[category])\n",
        "            \n",
        "        for label in entry.labels:\n",
        "            x = label.center_x * img_size\n",
        "            y = label.center_y * img_size\n",
        "            w = label.width * img_size\n",
        "            h = label.height * img_size\n",
        "            rect = plt.Rectangle((x - w / 2, y - h / 2), w, h, fill=False, color='r')\n",
        "            axs[idx, i].add_patch(rect)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(data: list[Entry]):\n",
        "    images = np.array([entry.image for entry in data])\n",
        "    labels = np.array([entry.labels[0].category for entry in data])\n",
        "    return images, labels\n",
        "\n",
        "train_images, train_labels = get_data(train_data)\n",
        "validate_images, validate_labels = get_data(validate_data)\n",
        "test_images, test_labels = get_data(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# validation accuracy would always stagnate at 75%, the model could go to 99% training accuracy with 4096 - 1024 - 256 but would overfit\n",
        "# that's why we need dropout but more neurons to compensate for the dropout, i don't wanna use l1/l2 or augmentation - just plain ann\n",
        "# training + validation accuracy would go to 70% with 6144 - 3072 - 1024 - 512 and 0.0001 lr, log curve and 150 epochs\n",
        "# now testing with 300 epochs and 0.0002 lr\n",
        "# lr 0.0002 didn't work, wonky and plateau at 50%, trying lr 0.0001 and 300 epochs -> still stopping at 70%\n",
        "# may try a different optimizer if my hyperparam search over 1e-2 - 1e-6 lr, 1-3 layers (0 - 0.5 dropout & 32-512 neurons each) doesnt work\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(img_size, img_size, 3)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# high complexity and with dropout\n",
        "model.add(Dense(6144, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3072, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(len(categories), activation='softmax'))\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# lowest complexity possible to reach an okay performance\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(len(categories), activation='softmax'))\n",
        "# model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=12, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(train_images, \n",
        "                    train_labels, \n",
        "                    epochs=10,\n",
        "                    batch_size=2000,\n",
        "                    validation_data=(validate_images, validate_labels), \n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "plt.plot(history.history['accuracy'], color='red', label='Training')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label='Validierung')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochen')\n",
        "plt.ylabel('Genauigkeit')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(img_size, img_size, 3)))\n",
        "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(8, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(categories), activation='softmax'))\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=10, validation_data=(validate_images, validate_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "plt.plot(history.history['accuracy'], color='red')\n",
        "plt.plot(history.history['val_accuracy'], color='blue')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the output value of the first layer of the custom model\n",
        "print(model.layers)\n",
        "\n",
        "layer_models = []\n",
        "for i, layer in enumerate(model.layers):\n",
        "    layer_model = Sequential()\n",
        "    for j in range(i + 1):\n",
        "        layer_model.add(model.layers[j])\n",
        "    layer_models.append(layer_model)\n",
        "\n",
        "layer_models[1].summary()\n",
        "\n",
        "# image = train_images[:1]\n",
        "# print(image.shape)\n",
        "# layer_models[1].predict(image)\n",
        "\n",
        "# raise\n",
        "\n",
        "# while True:\n",
        "#     # keras expects a batch of images, so we need to add an additional dimension to the image\n",
        "#     input_data = test_images[np.random.randint(0, len(test_images))][np.newaxis]\n",
        "#     layer_output = [layer_model.predict(input_data) for layer_model in layer_models]\n",
        "\n",
        "#     # the plot should look like this:\n",
        "#     #                        Layer 1 Filter 1 weights     Layer 1 Output 1       ...       Layer n Filter 1 weights     Layer n Output 1      \n",
        "#     # Original Image         ...                          ...                    ...       ...                          ...                     Predicted Category\n",
        "#     #                        Layer 1 Filter n weights     Layer 1 Output n       ...       Layer n Filter n weights     Layer n Output n\n",
        "#     #\n",
        "\n",
        "#     fig, ax, plt.subplots()\n",
        "\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "input_data = test_images[np.random.randint(0, len(test_images))][np.newaxis]\n",
        "layer_output = layer_models[0].predict(input_data)\n",
        "# plot the original image, the weights of the first kernel and the output of the first layer\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "axs[0].imshow(input_data[0])\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Original Image')\n",
        "\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "weights_min = weights.min()\n",
        "weights_max = weights.max()\n",
        "weights = (weights - weights_min) / (weights_max - weights_min)\n",
        "axs[1].imshow(weights[:, :, 0, 0], cmap='gray')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('First Kernel Weights')\n",
        "\n",
        "output = layer_output[0]\n",
        "axs[2].imshow(output[:, :, 0], cmap='gray')\n",
        "axs[2].axis('off')\n",
        "axs[2].set_title('Output of First Layer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What happened?\n",
        "1. I suffered from exploding gradients with HeInitialization() and MeanSquaredError(). I tried gradient clipping to 1.0 but turns out I don't have exploding gradients but vanishing gradients and exploding outputs. The outputs were very high with very low gradients, weird... Possible reasons are MeanSquaredError(), ReLU() and the batch size of 1 leading to instable gradients. Other potential reason: high learning rate.\n",
        "2. I changed to CrossEntropy() -> still vanishing gradients\n",
        "3. LeakyReLU() -> still vanishing\n",
        "4. Used RandomInitialization() and Sigmoid() activation function, works perfectly now\n",
        "\n",
        "I'm going to try adding batch processing so it will also work, maybe something with my Softmax output function was wrong, I want to try the original architecture (128 64 LeakyReLU, CrossEntropy, Xavier but with a different output activation function to see if the code for Softmax is broken)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
