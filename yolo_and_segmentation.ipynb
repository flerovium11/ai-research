{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLO and a Segmentation Model for road markings on Images from self-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset by splitting video into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, dims=(512, 512)):\n",
    "    image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # crop image to a centered square\n",
    "    min_dim = min(image.shape[0], image.shape[1])\n",
    "    start_x = (image.shape[1] - min_dim) // 2\n",
    "    start_y = (image.shape[0] - min_dim) // 2\n",
    "    image = image[start_y:start_y+min_dim, start_x:start_x+min_dim]\n",
    "\n",
    "    image = cv2.resize(image, dims)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Are you sure you want to regenerate the frames? Press Enter to continue...')\n",
    "print()\n",
    "\n",
    "videos_path = 'data/traffic-signs-and-road-markings'\n",
    "video_names = ['video1.mp4', 'video2.mp4']\n",
    "sampling_rate = 20\n",
    "output_path = 'data/traffic-signs-and-road-markings/frames'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print('Deleting existing frames...')\n",
    "    for file_name in os.listdir(output_path):\n",
    "        os.remove(os.path.join(output_path, file_name))\n",
    "else:\n",
    "    print('Creating output directory...')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print('Extracting frames...')\n",
    "for video_name in video_names:\n",
    "    video_path = os.path.join(videos_path, video_name)\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f'Video: {video_name}')\n",
    "    print(f'FPS: {fps}')\n",
    "    print(f'Size: {width}x{height}')\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    while True:\n",
    "        success, frame = capture.read()\n",
    "        print(f'Saved frames: {saved_frames}', end='\\r')\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        if frame_count % sampling_rate == 0:\n",
    "            saved_frames += 1\n",
    "            cv2.imwrite(os.path.join(output_path, f'{video_name.split('.')[0]}_{str(saved_frames).zfill(3)}.jpg'), preprocess(frame))\n",
    "                \n",
    "        frame_count += 1\n",
    "    \n",
    "    print(f'Successfully saved {saved_frames} frames from video {video_name} ({frame_count} frames in total)')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "dataset_path = 'data/traffic-signs-and-road-markings/sr-20-annotated'\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization-classes.txt'), 'r') as file:\n",
    "    localization_classes = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation-classes.txt'), 'r') as file:\n",
    "    segmentation_classes = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization-classes-german.txt'), 'r', encoding='utf-8') as file:\n",
    "    localization_classes_german = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation-classes-german.txt'), 'r', encoding='utf-8') as file:\n",
    "    segmentation_classes_german = file.read().splitlines()\n",
    "\n",
    "print('Localization classes:', localization_classes)\n",
    "print('Segmentation classes:', segmentation_classes)\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization.json'), 'r') as file:\n",
    "    localization_data_list = json.load(file)\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation.json'), 'r') as file:\n",
    "    segmentation_data_list = json.load(file)\n",
    "\n",
    "localization_data = {}\n",
    "for entry in localization_data_list:\n",
    "    image_name = entry['data']['image'].split('%5C')[-1]\n",
    "    localization_data[image_name] = entry\n",
    "\n",
    "segmentation_data = {}\n",
    "for entry in segmentation_data_list:\n",
    "    image_name = entry['data']['image'].split('%5C')[-1]\n",
    "    segmentation_data[image_name] = entry\n",
    "\n",
    "image_width = 512\n",
    "image_height = 512\n",
    "             \n",
    "data = []\n",
    "\n",
    "for image_name in os.listdir(images_path):\n",
    "    image_path = os.path.join(images_path, image_name)\n",
    "    image = plt.imread(image_path)\n",
    "    \n",
    "    if image.shape[0] != image_height or image.shape[1] != image_width:\n",
    "        print(f'Invalid image size: {image.shape} (expected: {image_height}x{image_width})')\n",
    "        continue\n",
    "    \n",
    "    localization = localization_data[image_name]\n",
    "    bounding_boxes = []\n",
    "    for annotation in localization['annotations']:\n",
    "        result = annotation['result']\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        for entry in result:\n",
    "            value = entry['value']\n",
    "            label = value['rectanglelabels'][0]\n",
    "            if label not in localization_classes:\n",
    "                print(f'Unknown localization class: {label}')\n",
    "                continue\n",
    "                \n",
    "            class_index = localization_classes.index(label)\n",
    "            x_center = (value['x'] + value['width'] / 2) / 100\n",
    "            y_center = (value['y'] + value['height'] / 2) / 100\n",
    "            width = value['width'] / 100\n",
    "            height = value['height'] / 100\n",
    "            bounding_boxes.append([class_index, x_center, y_center, width, height])\n",
    "\n",
    "    # create binary mask for each segmentation class from the polygon data\n",
    "    segmentation = segmentation_data[image_name]\n",
    "    masks = np.zeros((len(segmentation_classes), image_width, image_height), dtype=np.uint8)\n",
    "    for annotation in segmentation['annotations']:\n",
    "        result = annotation['result']\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        for entry in result:\n",
    "            value = entry['value']\n",
    "            label = value['polygonlabels'][0]\n",
    "            if label not in segmentation_classes:\n",
    "                print(f'Unknown segmentation class: {label}')\n",
    "                continue\n",
    "\n",
    "            class_index = segmentation_classes.index(label)\n",
    "            points = value['points']\n",
    "            # points are given as percentages of the image size\n",
    "            polygon = np.array([[int(p[0] / 100 * image_width), int(p[1] / 100 * image_height)] for p in points], dtype=np.int32)\n",
    "            cv2.fillPoly(masks[class_index], [polygon], 1)\n",
    "\n",
    "    data.append({\n",
    "        'name': image_name,\n",
    "        'image': image,\n",
    "        'bounding_boxes': bounding_boxes,\n",
    "        'segmentation_masks': masks\n",
    "    })\n",
    "\n",
    "# sort by frame because i had unlucky naming when labeling the data for the first time\n",
    "data.sort(key=lambda x: [int(n) if n.isdigit() else n for n in re.split(r'(\\d+)', x['name'])])\n",
    "\n",
    "print([d['name'] for d in data])\n",
    "print(f'Successfully loaded {len(data)} data entries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "colors = [\n",
    "    [0.1, 0.2, 0.5],\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.1, 0.6, 0.1],\n",
    "    [0.6, 0.1, 0.6],\n",
    "    [0.1, 0.6, 0.6],\n",
    "]\n",
    "\n",
    "def show_overlayed_masks(ax: plt.axes, masks: list[np.ndarray]):\n",
    "    surrounding_mask = np.ones((image_width, image_height), dtype=np.uint8)\n",
    "    for mask in masks:\n",
    "        surrounding_mask[mask == 1] = 0\n",
    "\n",
    "    display_masks = [*masks, surrounding_mask]\n",
    "    for i, mask in enumerate(display_masks):\n",
    "        overlay = np.zeros((image_width, image_height, 4))\n",
    "        overlay[mask == 1] = [*colors[i], 1]\n",
    "        ax.imshow(overlay, alpha=0.3)\n",
    "    \n",
    "    legend_elements = [\n",
    "        Patch(facecolor=[*colors[i], 0.6], label=[*segmentation_classes_german, 'Umgebung'][i]) for i in range(len(segmentation_classes) + 1)\n",
    "    ]\n",
    "\n",
    "    plt.legend(handles=legend_elements, loc='upper right', fontsize=6, title='Segmentierung', title_fontsize=8)\n",
    "\n",
    "def show_bounding_boxes(ax: plt.axes, bounding_boxes: list[list[float]]):\n",
    "    for box in bounding_boxes:\n",
    "        class_index, x_center, y_center, width, height = box\n",
    "        x = (x_center - width / 2) * image_width\n",
    "        y = (y_center - height / 2) * image_height\n",
    "        rect = plt.Rectangle((x, y), width * image_width, height * image_height, linewidth=1, edgecolor=colors[class_index], facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 5, y - 10, localization_classes_german[class_index], color='white', backgroundcolor=[c * 0.5 for c in colors[class_index]], fontsize=6)\n",
    "\n",
    "def save_annotated_frames(data: list[dict], folder: str = 'frames', overwrite: bool = False, show_masks: bool = True, show_boxes: bool = True):    \n",
    "    folder_path = os.path.join('media', folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        if not overwrite:\n",
    "            raise FileExistsError(f'Folder {folder_path} already exists')\n",
    "        \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            os.remove(os.path.join(folder_path, file_name))\n",
    "    else:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(entry['image'])\n",
    "        ax.axis('off')\n",
    "        if show_masks:\n",
    "            show_overlayed_masks(ax, entry['segmentation_masks'])\n",
    "        if show_boxes:\n",
    "            show_bounding_boxes(ax, entry['bounding_boxes'])\n",
    "        frame_path = os.path.join(folder_path, f'frame_{i}.png')\n",
    "        plt.savefig(frame_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        print(f'Saved frame {i}/{len(data)}', end='\\r')\n",
    "\n",
    "    print(f'Successfully saved {len(data)} frames')\n",
    "\n",
    "def join_frames_to_video(source_path = 'media/frames', target_path = 'media', video_name = 'video', fps = 2, dims = (512, 512)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_path = os.path.join(target_path, f'{video_name}.mp4')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, fps, dims, True)\n",
    "    files = os.listdir(source_path)\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        img = cv2.imread(os.path.join(source_path, file))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        if img.shape[0] != dims[0] or img.shape[1] != dims[1]:\n",
    "            img = cv2.resize(img, dims)\n",
    "        \n",
    "        out.write(img)\n",
    "        print(f'Processed frame {i}/{len(files)}', end='\\r')\n",
    "\n",
    "    out.release()\n",
    "    print(f'Video with {len(files)} frames saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_annotated_frames(data, show_masks=True, show_boxes=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_frames_to_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = data[75]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(entry['image'])\n",
    "ax.axis('off')\n",
    "show_overlayed_masks(ax, entry['segmentation_masks'])\n",
    "# show_bounding_boxes(ax, entry['bounding_boxes'])\n",
    "# plt.savefig('media/Segmentierung_Beispiel.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Pretrained YOLO Model\n",
    "https://docs.ultralytics.com/de/usage/python/\\\n",
    "https://pytorch.org/hub/ultralytics_yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'data/traffic-signs-and-road-markings'\n",
    "dataset_path = os.path.join(dataset_root, 'yolo-with-images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset exported from Labelstudio to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "labels_path = os.path.join(dataset_path, 'labels')\n",
    "\n",
    "# split data into val and train\n",
    "train_val_split = 0.8\n",
    "images_names = os.listdir(images_path)\n",
    "np.random.shuffle(images_names)\n",
    "split_index = int(len(images_names) * train_val_split)\n",
    "train_images_names = images_names[:split_index]\n",
    "val_images_names = images_names[split_index:]\n",
    "\n",
    "if os.path.exists(os.path.join(dataset_path, 'train')):\n",
    "  shutil.rmtree(os.path.join(dataset_path, 'train'))\n",
    "if os.path.exists(os.path.join(dataset_path, 'val')):\n",
    "  shutil.rmtree(os.path.join(dataset_path, 'val'))\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'labels'), exist_ok=True)\n",
    "\n",
    "for image_name in train_images_names:\n",
    "    shutil.copy(os.path.join(images_path, image_name), os.path.join(dataset_path, 'train', 'images'))\n",
    "    shutil.copy(os.path.join(labels_path, image_name.replace('.jpg', '.txt')), os.path.join(dataset_path, 'train', 'labels'))\n",
    "\n",
    "for image_name in val_images_names:\n",
    "    shutil.copy(os.path.join(images_path, image_name), os.path.join(dataset_path, 'val', 'images'))\n",
    "    shutil.copy(os.path.join(labels_path, image_name.replace('.jpg', '.txt')), os.path.join(dataset_path, 'val', 'labels'))\n",
    "\n",
    "# generate dataset.yaml\n",
    "dataset_yaml = f\"\"\"\n",
    "train: ./train/images\n",
    "val: ./val/images\n",
    "\n",
    "nc: {len(localization_classes)}\n",
    "names: {{\"{', '.join(localization_classes)}\"}}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(dataset_path, 'dataset.yaml'), 'w') as file:\n",
    "    file.write(dataset_yaml)\n",
    "  \n",
    "print('Successfully split data into train and val datasets and generated dataset.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('models/yolov5su.pt')\n",
    "model.train(data=os.path.join(dataset_path, 'dataset.yaml'), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(dataset_path, 'images')\n",
    "for image_name in os.listdir(data_path):\n",
    "    results = model.predict(source=os.path.join(data_path, image_name))\n",
    "    plt.imshow(results[0].plot())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate Video with YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 1\n",
    "dims = (512, 512)\n",
    "video_name = 'video1.mp4'\n",
    "video_path = os.path.join(dataset_root, video_name)\n",
    "capture = cv2.VideoCapture(video_path)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'FPS: {fps}')\n",
    "print(f'Size: {width}x{height}')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = os.path.join(dataset_root, f'{video_name.split(\".\")[0]}_yolo.mp4')\n",
    "out = cv2.VideoWriter(output_path, fourcc, int(fps / sampling_rate), dims, True)\n",
    "\n",
    "frame_count = 0\n",
    "saved_frames = 0\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    if frame_count % sampling_rate == 0:\n",
    "        saved_frames += 1\n",
    "        results = model.predict(source=preprocess(frame, dims), save=False)\n",
    "        image = results[0].plot()\n",
    "        out.write(image)\n",
    "        print(f'Processed frames: {saved_frames}', end='\\r')\n",
    "       \n",
    "    frame_count += 1\n",
    "  \n",
    "print('Successfully generated video')\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Modellaufbau\n",
    "input_layer = layers.Input(shape=(512, 512, 3))\n",
    "\n",
    "# Encoder: Convolutional Layers\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Decoder: Upsampling und Convolution\n",
    "x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Ausgangsschicht für 2 Masken\n",
    "output_layer = layers.Conv2D(2, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Modell erstellen\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.8\n",
    "split_index = int(len(data) * train_val_split)\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)\n",
    "\n",
    "train_images = np.array([entry['image'] for entry in train_data])\n",
    "train_masks = np.array([np.transpose(entry['segmentation_masks'], (1, 2, 0)) for entry in train_data])\n",
    "val_images = np.array([entry['image'] for entry in val_data])\n",
    "val_masks = np.array([np.transpose(entry['segmentation_masks'], (1, 2, 0)) for entry in val_data])\n",
    "\n",
    "model.fit(train_images, train_masks, epochs=5, validation_data=(val_images, val_masks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
