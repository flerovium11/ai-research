{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLO and a Segmentation Model for road markings on Images from self-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, dims=(512, 512)):\n",
    "    image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    # crop image to a centered square\n",
    "    min_dim = min(image.shape[0], image.shape[1])\n",
    "    start_x = (image.shape[1] - min_dim) // 2\n",
    "    start_y = (image.shape[0] - min_dim) // 2\n",
    "    image = image[start_y:start_y+min_dim, start_x:start_x+min_dim]\n",
    "\n",
    "    image = cv2.resize(image, dims)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset by splitting video into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Are you sure you want to regenerate the frames? Press Enter to continue...')\n",
    "print()\n",
    "\n",
    "videos_path = 'data/traffic-signs-and-road-markings'\n",
    "video_names = ['video1.mp4', 'video2.mp4']\n",
    "skip_frames = 20\n",
    "output_path = 'data/traffic-signs-and-road-markings/frames'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print('Deleting existing frames...')\n",
    "    for file_name in os.listdir(output_path):\n",
    "        os.remove(os.path.join(output_path, file_name))\n",
    "else:\n",
    "    print('Creating output directory...')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print('Extracting frames...')\n",
    "for video_name in video_names:\n",
    "    video_path = os.path.join(videos_path, video_name)\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f'Video: {video_name}')\n",
    "    print(f'FPS: {fps}')\n",
    "    print(f'Size: {width}x{height}')\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    while True:\n",
    "        success, frame = capture.read()\n",
    "        print(f'Saved frames: {saved_frames}', end='\\r')\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        if frame_count % skip_frames == 0:\n",
    "            saved_frames += 1\n",
    "            cv2.imwrite(os.path.join(output_path, f'{video_name.split(\".\")[0]}_{str(saved_frames).zfill(3)}.jpg'), preprocess(frame))\n",
    "                \n",
    "        frame_count += 1\n",
    "    \n",
    "    print(f'Successfully saved {saved_frames} frames from video {video_name} ({frame_count} frames in total)')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "dataset_root = 'data/traffic-signs-and-road-markings'\n",
    "dataset_path = 'data/traffic-signs-and-road-markings/sr-20-annotated'\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization-classes.txt'), 'r') as file:\n",
    "    localization_classes = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation-classes.txt'), 'r') as file:\n",
    "    segmentation_classes = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization-classes-german.txt'), 'r', encoding='utf-8') as file:\n",
    "    localization_classes_german = file.read().splitlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation-classes-german.txt'), 'r', encoding='utf-8') as file:\n",
    "    segmentation_classes_german = file.read().splitlines()\n",
    "\n",
    "print('Localization classes:', localization_classes)\n",
    "print('Segmentation classes:', segmentation_classes)\n",
    "\n",
    "with open(os.path.join(dataset_path, 'localization.json'), 'r') as file:\n",
    "    localization_data_list = json.load(file)\n",
    "\n",
    "with open(os.path.join(dataset_path, 'segmentation.json'), 'r') as file:\n",
    "    segmentation_data_list = json.load(file)\n",
    "\n",
    "localization_data = {}\n",
    "for entry in localization_data_list:\n",
    "    image_name = entry['data']['image'].split('%5C')[-1]\n",
    "    localization_data[image_name] = entry\n",
    "\n",
    "segmentation_data = {}\n",
    "for entry in segmentation_data_list:\n",
    "    image_name = entry['data']['image'].split('%5C')[-1]\n",
    "    segmentation_data[image_name] = entry\n",
    "\n",
    "image_width = 512\n",
    "image_height = 512\n",
    "             \n",
    "data = []\n",
    "\n",
    "for image_name in os.listdir(images_path):\n",
    "    image_path = os.path.join(images_path, image_name)\n",
    "    image = plt.imread(image_path)\n",
    "    \n",
    "    if image.shape[0] != image_height or image.shape[1] != image_width:\n",
    "        print(f'Invalid image size: {image.shape} (expected: {image_height}x{image_width})')\n",
    "        continue\n",
    "    \n",
    "    localization = localization_data[image_name]\n",
    "    bounding_boxes = []\n",
    "    for annotation in localization['annotations']:\n",
    "        result = annotation['result']\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        for entry in result:\n",
    "            value = entry['value']\n",
    "            label = value['rectanglelabels'][0]\n",
    "            if label not in localization_classes:\n",
    "                print(f'Unknown localization class: {label}')\n",
    "                continue\n",
    "                \n",
    "            class_index = localization_classes.index(label)\n",
    "            x_center = (value['x'] + value['width'] / 2) / 100\n",
    "            y_center = (value['y'] + value['height'] / 2) / 100\n",
    "            width = value['width'] / 100\n",
    "            height = value['height'] / 100\n",
    "            bounding_boxes.append([class_index, x_center, y_center, width, height])\n",
    "\n",
    "    # create binary mask for each segmentation class from the polygon data\n",
    "    segmentation = segmentation_data[image_name]\n",
    "    masks = np.zeros((len(segmentation_classes), image_width, image_height), dtype=np.uint8)\n",
    "    for annotation in segmentation['annotations']:\n",
    "        result = annotation['result']\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        for entry in result:\n",
    "            value = entry['value']\n",
    "            label = value['polygonlabels'][0]\n",
    "            if label not in segmentation_classes:\n",
    "                print(f'Unknown segmentation class: {label}')\n",
    "                continue\n",
    "\n",
    "            class_index = segmentation_classes.index(label)\n",
    "            points = value['points']\n",
    "            # points are given as percentages of the image size\n",
    "            polygon = np.array([[int(p[0] / 100 * image_width), int(p[1] / 100 * image_height)] for p in points], dtype=np.int32)\n",
    "            cv2.fillPoly(masks[class_index], [polygon], 1)\n",
    "\n",
    "    data.append({\n",
    "        'name': image_name,\n",
    "        'image': image,\n",
    "        'bounding_boxes': bounding_boxes,\n",
    "        'segmentation_masks': masks\n",
    "    })\n",
    "\n",
    "# sort by frame because i had unlucky naming when labeling the data for the first time\n",
    "data.sort(key=lambda x: [int(n) if n.isdigit() else n for n in re.split(r'(\\d+)', x['name'])])\n",
    "\n",
    "print([d['name'] for d in data])\n",
    "print(f'Successfully loaded {len(data)} data entries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "colors = [\n",
    "    [0.1, 0.2, 0.5],\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.1, 0.6, 0.1],\n",
    "    [0.6, 0.1, 0.6],\n",
    "    [0.1, 0.6, 0.6],\n",
    "]\n",
    "\n",
    "def show_overlayed_masks(ax: plt.axes, masks: list[np.ndarray], alpha: float = 0.3, legend_fontsize: int = 6, legend_title_fontsize: int = 8):\n",
    "    width, height = masks[0].shape\n",
    "    surrounding_mask = np.ones((width, height), dtype=np.uint8)\n",
    "    for mask in masks:\n",
    "        surrounding_mask[mask > 0] = 0\n",
    "\n",
    "    display_masks = [*masks, surrounding_mask]\n",
    "    for i, mask in enumerate(display_masks):\n",
    "        overlay = np.zeros((mask.shape[0], mask.shape[1], 4), dtype=np.float32)\n",
    "        overlay[..., :3] = colors[i]  \n",
    "        overlay[..., 3] = mask\n",
    "        ax.imshow(overlay, alpha=alpha)\n",
    "    \n",
    "    legend_elements = [\n",
    "        Patch(facecolor=[*colors[i], 0.6], label=[*segmentation_classes_german, 'Umgebung'][i]) for i in range(len(segmentation_classes) + 1)\n",
    "    ]\n",
    "\n",
    "    plt.legend(handles=legend_elements, loc='upper right', fontsize=legend_fontsize, title='Segmentierung', title_fontsize=legend_title_fontsize)\n",
    "\n",
    "def show_bounding_boxes(ax: plt.axes, bounding_boxes: list[list[float]], linewidth: float = 1, fontsize: int = 6):\n",
    "    for box in bounding_boxes:\n",
    "        class_index, x_center, y_center, width, height = box\n",
    "        x = (x_center - width / 2) * image_width\n",
    "        y = (y_center - height / 2) * image_height\n",
    "        rect = plt.Rectangle((x, y), width * image_width, height * image_height, linewidth=linewidth, edgecolor=colors[class_index], facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 5, y - 10, localization_classes_german[class_index], color='white', backgroundcolor=[c * 0.5 for c in colors[class_index]], fontsize=fontsize)\n",
    "\n",
    "def save_data_as_annotated_frames(data: list[dict], folder: str = 'frames', overwrite: bool = False, show_masks: bool = True, show_boxes: bool = True):    \n",
    "    folder_path = os.path.join('media', folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        if not overwrite:\n",
    "            raise FileExistsError(f'Folder {folder_path} already exists')\n",
    "        \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            os.remove(os.path.join(folder_path, file_name))\n",
    "    else:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(entry['image'])\n",
    "        ax.axis('off')\n",
    "        if show_masks:\n",
    "            show_overlayed_masks(ax, entry['segmentation_masks'])\n",
    "        if show_boxes:\n",
    "            show_bounding_boxes(ax, entry['bounding_boxes'])\n",
    "        frame_path = os.path.join(folder_path, f'frame_{i}.png')\n",
    "        plt.savefig(frame_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        print(f'Saved frame {i}/{len(data)}', end='\\r')\n",
    "\n",
    "    print(f'Successfully saved {len(data)} frames')\n",
    "\n",
    "def join_frames_to_video(source_path = 'media/frames', target_path = 'media', video_name = 'video', fps = 2, dims = (512, 512)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_path = os.path.join(target_path, f'{video_name}.mp4')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, fps, dims, True)\n",
    "    files = os.listdir(source_path)\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        img = cv2.imread(os.path.join(source_path, file))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        if img.shape[0] != dims[0] or img.shape[1] != dims[1]:\n",
    "            img = cv2.resize(img, dims)\n",
    "        \n",
    "        out.write(img)\n",
    "        print(f'Processed frame {i}/{len(files)}', end='\\r')\n",
    "\n",
    "    out.release()\n",
    "    print(f'Video with {len(files)} frames saved successfully')\n",
    "\n",
    "def annotate_frame_with_yolo(model: any, frame: any, dims: tuple[int] = (512, 512)):\n",
    "    results = model.predict(source=preprocess(frame, dims), save=False)\n",
    "    image = results[0].plot()\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def annotate_plot_with_segmentation_models(models: list[any], ax: plt.axes, image: any, dims: tuple[int] = (512, 512), **kwargs):\n",
    "    masks = []\n",
    "    for model in models:\n",
    "        predicted_masks = model.predict(np.expand_dims(image, axis=0), verbose=0)[0]\n",
    "        predicted_masks = np.transpose(predicted_masks, (2, 0, 1))\n",
    "        predicted_masks = [cv2.resize(mask, dims) for mask in predicted_masks]\n",
    "        masks.extend(predicted_masks)\n",
    "    \n",
    "    show_overlayed_masks(ax, masks, **kwargs)\n",
    "    \n",
    "def annotate_video(input_path: str, output_path: str, process_frame: callable, dims: tuple[int] = (512, 512), sr: int = 1, max_frames = None):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise OSError('Input path does not exist')\n",
    "    capture = cv2.VideoCapture(input_path)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print('Annotating video')\n",
    "    print(f'FPS: {fps}')\n",
    "    print(f'Size: {width}x{height}')\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, int(fps / sr), dims, True)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    while True:\n",
    "        success, frame = capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        if frame_count % sr == 0:\n",
    "            saved_frames += 1\n",
    "            out.write(process_frame(frame))\n",
    "            print(f'Processed frames: {saved_frames}', end='\\r')\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "        if max_frames is not None and saved_frames >= max_frames:\n",
    "            break\n",
    "    \n",
    "    print('Successfully generated video')\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = data[75]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(entry['image'])\n",
    "ax.axis('off')\n",
    "show_overlayed_masks(ax, entry['segmentation_masks'])\n",
    "# show_bounding_boxes(ax, entry['bounding_boxes'])\n",
    "# plt.savefig('media/Segmentierung_Beispiel.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 2, 3\n",
    "fig, ax = plt.subplots(n, m, figsize=(m * 3, n * 3))\n",
    "np.random.shuffle(data)\n",
    "for x in range(n):\n",
    "    for y in range(m):\n",
    "        ax[x, y].axis('off')\n",
    "        entry = data[y * 10 + x]\n",
    "        ax[x, y].imshow(entry['image'])\n",
    "        # show_bounding_boxes(ax[x, y], entry['bounding_boxes'], linewidth=2, fontsize=8)\n",
    "        show_overlayed_masks(ax[x, y], entry['segmentation_masks'], legend_fontsize=10, legend_title_fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('media/Auszug_Eigenes_Datenset_Segmentierung.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = data[75]\n",
    "# 3 images side by side, original, mask 0, mask 1\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(entry['image'])\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(entry['segmentation_masks'][0], cmap='viridis')\n",
    "axs[1].set_title('Maske für \"Straße\"')\n",
    "axs[1].axis('off')\n",
    "axs[2].imshow(entry['segmentation_masks'][1], cmap='viridis')\n",
    "axs[2].set_title('Maske für \"Straßenschild\"')\n",
    "axs[2].axis('off')\n",
    "# plt.savefig('media/Segmentierungsmasken.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Pretrained YOLO Model\n",
    "https://docs.ultralytics.com/de/usage/python/\\\n",
    "https://pytorch.org/hub/ultralytics_yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(dataset_root, 'yolo-format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset exported from Labelstudio to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "labels_path = os.path.join(dataset_path, 'labels')\n",
    "\n",
    "# split data into val and train\n",
    "train_val_split = 0.8\n",
    "images_names = os.listdir(images_path)\n",
    "np.random.shuffle(images_names)\n",
    "split_index = int(len(images_names) * train_val_split)\n",
    "train_images_names = images_names[:split_index]\n",
    "val_images_names = images_names[split_index:]\n",
    "\n",
    "if os.path.exists(os.path.join(dataset_path, 'train')):\n",
    "  shutil.rmtree(os.path.join(dataset_path, 'train'))\n",
    "if os.path.exists(os.path.join(dataset_path, 'val')):\n",
    "  shutil.rmtree(os.path.join(dataset_path, 'val'))\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'labels'), exist_ok=True)\n",
    "\n",
    "for image_name in train_images_names:\n",
    "    shutil.copy(os.path.join(images_path, image_name), os.path.join(dataset_path, 'train', 'images'))\n",
    "    shutil.copy(os.path.join(labels_path, image_name.replace('.jpg', '.txt')), os.path.join(dataset_path, 'train', 'labels'))\n",
    "\n",
    "for image_name in val_images_names:\n",
    "    shutil.copy(os.path.join(images_path, image_name), os.path.join(dataset_path, 'val', 'images'))\n",
    "    shutil.copy(os.path.join(labels_path, image_name.replace('.jpg', '.txt')), os.path.join(dataset_path, 'val', 'labels'))\n",
    "\n",
    "# generate config.yaml\n",
    "names_string = ','.join([f'\"{class_name}\"' for class_name in localization_classes])\n",
    "dataset_yaml = f\"\"\"\n",
    "train: ./train/images\n",
    "val: ./val/images\n",
    "\n",
    "nc: {len(localization_classes)}\n",
    "names: [{names_string}]\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(dataset_path, 'config.yaml'), 'w') as file:\n",
    "    file.write(dataset_yaml)\n",
    "  \n",
    "print('Successfully split data into train and val datasets and generated config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('models/yolo11m.pt')\n",
    "results = model.train(data=os.path.join(dataset_path, 'config.yaml'), epochs=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('models/yolo5su_fine_tuned.pt')\n",
    "results = model.val(data=os.path.join(dataset_path, 'config.yaml'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(dataset_path, 'images')\n",
    "for image_name in os.listdir(data_path)[:5]:\n",
    "    results = model.predict(source=os.path.join(data_path, image_name))\n",
    "    image = results[0].plot()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate Video with YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'video.mp4'\n",
    "video_path = os.path.join(dataset_root, video_name)\n",
    "output_path = os.path.join(dataset_root, f'{video_name.split(\".\")[0]}_yolo.mp4')\n",
    "\n",
    "def process_frame(frame):\n",
    "    return annotate_frame_with_yolo(model, frame, (512, 512))\n",
    "\n",
    "annotate_video(video_path, output_path, process_frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_size = 128\n",
    "train_val_split = 0.8\n",
    "split_index = int(len(data) * train_val_split)\n",
    "np.random.shuffle(data)\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "\n",
    "def images_from_data(data: list[dict]) -> np.ndarray:\n",
    "    return np.array([cv2.resize(entry['image'], (img_size, img_size)) for entry in data]) / 255.0\n",
    "\n",
    "def masks_from_data(data: list[dict], channel: int) -> np.ndarray:\n",
    "    return np.expand_dims(np.array([cv2.resize(entry['segmentation_masks'][channel], (img_size, img_size)) for entry in data]), axis=-1)\n",
    "\n",
    "# Data augmentation configuration\n",
    "image_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "mask_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Generator function to yield augmented image and mask pairs\n",
    "def augment_data(image_batch, mask_batch):\n",
    "    # Using .flow() to create a data generator for images and masks\n",
    "    image_gen = image_datagen.flow(image_batch, batch_size=16, seed=42)\n",
    "    mask_gen = mask_datagen.flow(mask_batch, batch_size=16, seed=42)\n",
    "\n",
    "    # Combine the two generators\n",
    "    while True:\n",
    "        # The `next()` function gives a batch of augmented images and masks\n",
    "        img_batch = next(image_gen)\n",
    "        msk_batch = next(mask_gen)\n",
    "\n",
    "        yield img_batch, msk_batch\n",
    "\n",
    "# Convert your train data to numpy arrays\n",
    "train_images, train_road_masks, train_sign_masks = images_from_data(train_data), masks_from_data(train_data, 0), masks_from_data(train_data, 1)\n",
    "val_images, val_road_masks, val_sign_masks = images_from_data(val_data), masks_from_data(val_data, 0), masks_from_data(val_data, 1)\n",
    "\n",
    "# Now you can use this augmented data generator during training\n",
    "train_road_generator = augment_data(train_images, train_road_masks)\n",
    "train_sign_generator = augment_data(train_images, train_sign_masks)\n",
    "for i in range(1):\n",
    "    # Sample: Visualize some augmented images\n",
    "    sample_img, sample_mask = next(train_road_generator)\n",
    "\n",
    "    # Displaying one image and its corresponding mask\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(sample_img[0])\n",
    "    plt.axis('off')\n",
    "    plt.title('Augmented Image')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(sample_mask[0, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Augmented Mask')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model\\\n",
    "*Ended up not using augmentation from above, didn't improve accuracy*\\\n",
    "*The learning curves are extremely unstable, not enough data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# weight for traffic signs\n",
    "class_weight = 0.05\n",
    "tf.keras.saving.get_custom_objects().clear()\n",
    "\n",
    "@tf.keras.saving.register_keras_serializable()\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = y_true * (1 - class_weight) + (1 - y_true) * class_weight\n",
    "    return tf.keras.backend.mean(tf.keras.backend.binary_crossentropy(y_true, y_pred) * weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Encoder: Convolutional Layers\n",
    "model.add(layers.Input(shape=(img_size, img_size, 3)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "# # Decoder: Upsampling and Convolution\n",
    "model.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=weighted_binary_crossentropy, metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_sign_masks, epochs=150, validation_data=(val_images, val_sign_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], color='red', label='Training')\n",
    "plt.plot(history.history['val_accuracy'], color='blue', label='Validierung')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochen')\n",
    "# plt.ylim(0)\n",
    "# plt.gca().xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "# plt.savefig('media/Segmentierung_Straßenschild_Training_Diagramm.png')\n",
    "plt.ylabel('Genauigkeit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = val_images[11]\n",
    "predicted_masks = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "predicted_masks = np.transpose(predicted_masks, (2, 0, 1))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "show_overlayed_masks(ax, predicted_masks, alpha=1)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(predicted_masks[0], cmap='gray')\n",
    "# plt.show()\n",
    "# for row in predicted_masks[0]:\n",
    "#     print(' '.join([str(round(num, 2)) for num in row]))\n",
    "\n",
    "# for row in val_masks[0, :, :]:\n",
    "#     print(' '.join([str(round(num, 2)) for num in row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate Video with Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.saving import load_model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "road_model = load_model('models/road_segmentation_97_val_acc.keras')\n",
    "sign_model = load_model('models/sign_segmentation_97_val_acc.keras', custom_objects={'loss': weighted_binary_crossentropy})\n",
    "yolo = YOLO('models/yolov5su_fine_tuned.pt')\n",
    "\n",
    "video_name = 'video1.mp4'\n",
    "video_path = os.path.join(dataset_root, video_name)\n",
    "output_path = os.path.join(dataset_root, f'{video_name.split(\".\")[0]}_segmentations.mp4')\n",
    "\n",
    "# capture = cv2.VideoCapture(video_path)\n",
    "# for i in range(1):\n",
    "#     dims = (512, 512)\n",
    "#     fig, ax = plt.subplots()\n",
    "#     image = preprocess(frame, dims)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     ax.imshow(image)\n",
    "#     annotate_plot_with_segmentation_models([road_model, sign_model], ax, cv2.resize(image, sign_model.layers[0].output.shape[1:3]) / 255, alpha=0.7)\n",
    "#     ax.axis('off')\n",
    "#     plt.savefig('images/temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "#     cv2.cvtColor(cv2.resize(plt.imread('images/temp_frame.png'), dims), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def process_frame(frame):\n",
    "    dims = (512, 512)\n",
    "    fig, ax = plt.subplots()\n",
    "    image = annotate_frame_with_yolo(yolo, frame, dims)\n",
    "    ax.imshow(image)\n",
    "    annotate_plot_with_segmentation_models([road_model, sign_model], ax, cv2.resize(cv2.cvtColor(preprocess(frame, dims), cv2.COLOR_BGR2RGB), sign_model.layers[0].output.shape[1:3]) / 255, alpha=0.5)\n",
    "    ax.axis('off')\n",
    "    plt.savefig('images/temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return cv2.resize(cv2.imread('images/temp_frame.png'), dims)\n",
    "\n",
    "annotate_video(video_path, output_path, process_frame)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
